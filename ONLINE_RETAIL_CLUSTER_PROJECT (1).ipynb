{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -ONLINE RETAIL\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Cluster\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -**          - Sana Fatima\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**\n",
        "\n",
        "This project aims to perform customer segmentation for an online retail store by applying clustering techniques to its transactional dataset. The goal is to identify distinct customer groups based on their purchasing behaviors, demographics, and other relevant attributes.\n",
        "\n",
        "This information can then be used to tailor marketing strategies, personalize customer experiences, and ultimately improve business performance."
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "\n",
        "The online retail industry is highly competitive, and businesses need to understand their customers deeply to thrive. Traditional marketing approaches often treat all customers the same, leading to inefficient campaigns and missed opportunities. This project aims to address this challenge by leveraging customer segmentation through clustering.\n",
        "\n",
        "This problem statement sets the stage for your project by clearly defining the challenge, proposed solution, and desired outcomes. It provides a framework for your analysis and guides your efforts towards achieving the project objectives"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attribute information**\n",
        "\n",
        "1.InvoiceNo: A unique identifier for each transaction.\n",
        "\n",
        "2.StockCode: A unique identifier for each product.\n",
        "\n",
        "3.Description: A textual description of the product.\n",
        "\n",
        "4.Quantity: The number of units of a product purchased in a single transaction.\n",
        "\n",
        "5.InvoiceDate: The date and time of the transaction.\n",
        "\n",
        "6.UnitPrice: The price of a single unit of the product in the currency (e.g., Sterling).\n",
        "\n",
        "7.CustomerID: A unique identifier for each customer.\n",
        "\n",
        "8.Country: The country where the customer resides."
      ],
      "metadata": {
        "id": "nLbYEK6Un9ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import difflib"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('/content/Online Retail.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "lDPBVbYzp86b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated()"
      ],
      "metadata": {
        "id": "dXilUPgIGpG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows, keeping the first occurrence\n",
        "data = df.drop_duplicates()\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "5kcnZRZSG6or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description and CustomerID have null Values**"
      ],
      "metadata": {
        "id": "xpSZvjMBgKhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#null value in percentage\n",
        "for col in df.columns:\n",
        "  print(f\"{col} : Count : {df[col].isnull().sum()} : Percentage : {round(df[col].isnull().sum()/df.shape[0]*100, 2)}\")\n"
      ],
      "metadata": {
        "id": "q-IHqx2vqHGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove null value\n",
        "df['Description'].fillna('No Description', inplace=True)"
      ],
      "metadata": {
        "id": "9d900zwQhT44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove customerid\n",
        "df.drop('CustomerID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "JpB2X4eyhi4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "tDARHIErhtUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "DlG-53zEiCpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(10)"
      ],
      "metadata": {
        "id": "xmU7ZiRst_AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "FpRT2u_CknLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing null values in 'director' column with 'unknown'\n",
        "df['Description'].replace(np.nan, \"unknown\",inplace  = True)"
      ],
      "metadata": {
        "id": "Jv-LTbqikzv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.notnull().sum()"
      ],
      "metadata": {
        "id": "oeWX8WUtljzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Country\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "rk0OZ2Ly9Zl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Calculate country distribution\n",
        "country_distribution = df[\"Country\"].value_counts(normalize=True)\n",
        "\n",
        "# Create the bar chart\n",
        "fig = px.bar(\n",
        "    x=country_distribution.index,  # Country names on the x-axis\n",
        "    y=country_distribution.values,  # Normalized counts on the y-axis\n",
        "    title=\"Distribution of Countries\",\n",
        "    labels={\"x\": \"Country\", \"y\": \"Normalized Count\"},  # Add axis labels for clarity\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "m1MTlsELICPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert \"InvoiceNo\" to a string type series\n",
        "df[\"InvoiceNo\"] = df.InvoiceNo.astype(\"str\")\n",
        "# Convert \"Description\" to a string type series and remove extra whitespaces\n",
        "df[\"Description\"] = df.Description.astype(\"str\")\n",
        "df[\"Description\"] = df.Description.str.strip()\n"
      ],
      "metadata": {
        "id": "h3nRkQ7d_vPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy for plot\n",
        "products=data.copy()\n",
        "\n",
        "#removing unknown\n",
        "StockCode=products[products['StockCode']!='unknown']"
      ],
      "metadata": {
        "id": "5zwrq8qaKY85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for top 10 products\n",
        "\n",
        "plt.figure(figsize = (14,6))\n",
        "sns.countplot(y='StockCode',data=StockCode,order=StockCode.StockCode.value_counts().head(10).index,palette=\"gist_rainbow\")\n",
        "plt.title('produts with most stockcode',fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JfWQOW9CLLwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Products Cancellation count with country"
      ],
      "metadata": {
        "id": "ZOcaddilOEQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your original DataFrame\n",
        "\n",
        "# Filter rows where InvoiceNo starts with 'C' indicating cancellation\n",
        "canceled_orders = df[df['InvoiceNo'].str.startswith('C')]\n",
        "\n",
        "# Group by Country and StockCode to count cancellations for each product in each country\n",
        "country_product_cancellations = canceled_orders.groupby(['Country', 'StockCode'])['InvoiceNo'].count().reset_index()\n",
        "\n",
        "# Rename the count column for clarity\n",
        "country_product_cancellations.rename(columns={'InvoiceNo': 'CancellationCount'}, inplace=True)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(country_product_cancellations)\n"
      ],
      "metadata": {
        "id": "_MnKs_ETNOcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total cancellations per country\n",
        "country_cancellations = country_product_cancellations.groupby('Country')['CancellationCount'].sum().reset_index()\n",
        "\n",
        "# Sort countries by cancellation count in descending order\n",
        "country_cancellations = country_cancellations.sort_values('CancellationCount', ascending=False)"
      ],
      "metadata": {
        "id": "2G02Fg3HO-DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ploting the graph a country whose cancellation  is high"
      ],
      "metadata": {
        "id": "6mLGkdBOPKI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the bar graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Country', y='CancellationCount', data=country_cancellations, order=country_cancellations['Country'])\n",
        "top_country = country_cancellations.iloc[0]['Country']\n",
        "plt.title(f'Product Cancellations by Country (Highest in {top_country})')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Cancellation Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DSd2_X-WOajJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We will create a wordcloud to see which words appear the most in the titles for Description column"
      ],
      "metadata": {
        "id": "tKgeICmpTg2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(25,15))\n",
        "# parameters for wordcloud\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='Black',\n",
        "                          stopwords=set(STOPWORDS),\n",
        "                          max_words=1000,\n",
        "                          max_font_size=50,\n",
        "                          random_state=42,\n",
        "                          width=1920,\n",
        "                          height=1080\n",
        "                         ).generate(\" \".join(data['Description'].astype(str)))\n",
        "# Plot the image\n",
        "plt.title('Most used words in Stockcode column', fontsize = 20, pad=25)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.savefig('products.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8vsyILWaTgoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis on 'Country' column:-\n",
        "So we create wordcloud for country column"
      ],
      "metadata": {
        "id": "7vQL8ZaDVsNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(25,15))\n",
        "# parameters for wordcloud\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          width=1920,\n",
        "                          height=1080\n",
        "                         ).generate(\" \".join(data['Description'].astype(str))) # Join with space, convert to string\n",
        "# Plot the image\n",
        "plt.title('Most used words in Country', fontsize = 20, pad=25)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.savefig('Country.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SL_pIUEcVOW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering & Data Pre-processing"
      ],
      "metadata": {
        "id": "Irbd5Jn4XICj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering\n",
        "we will add all text based or categorical columns"
      ],
      "metadata": {
        "id": "LMMJzaVFXUoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will add all categorical and text based columns\n",
        "data['text_info'] = data['Country'].astype(str)"
      ],
      "metadata": {
        "id": "wwrb__sgXb1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking\n",
        "data['text_info'][0]"
      ],
      "metadata": {
        "id": "3sajGkFuYRYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text cleaning"
      ],
      "metadata": {
        "id": "RIZSDDxEYfeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text cleaning function\n",
        "import re\n",
        "def clean_text(x):\n",
        "    return re.sub(r\"[^a-zA-Z ]\",\"\",str(x))"
      ],
      "metadata": {
        "id": "AUpmDBQYYlBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying above function on our combined column\n",
        "data['text_info'] = data['text_info'].apply(clean_text)"
      ],
      "metadata": {
        "id": "WBSV95iFYo5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will convert all words in lowercase\n",
        "data['text_info'] = data['text_info'].str.lower()"
      ],
      "metadata": {
        "id": "mCL26mU2Ys7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary import for nlp\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "TF12zmI1Y7B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming-"
      ],
      "metadata": {
        "id": "dVeDw96oZCfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "9zptPwGUZEqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to filter the words\n",
        "def filter_words(string, filter_words):\n",
        "  filtered=[]\n",
        "  tokens = word_tokenize(string)\n",
        "  for word in tokens:\n",
        "    if word not in filter_words:\n",
        "      filtered.append(stemmer.stem(word))\n",
        "  return filtered\n",
        "\n",
        "data['cleaned_text']= ''\n",
        "for item, row in data.iterrows():\n",
        "  data.at[item,'cleaned_text'] = filter_words(row['text_info'],stop_words)\n",
        "\n",
        "data['cleaned_text']"
      ],
      "metadata": {
        "id": "uktynO61ZMdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LptQBSo01x2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#join words fun\n",
        "def join_words(x):\n",
        "  return \" \".join(x)"
      ],
      "metadata": {
        "id": "H0Aub7V6cx_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final column\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(join_words)"
      ],
      "metadata": {
        "id": "WzG9TAXMZ8MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(4)"
      ],
      "metadata": {
        "id": "RkEdruAJaAR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = data.cleaned_text\n",
        "words"
      ],
      "metadata": {
        "id": "gr_KAZcEaHKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# using TF-IDF\n",
        "\n",
        "Term frequency-inverse document frequency is a text vectorizer that transforms the text into a usable vector."
      ],
      "metadata": {
        "id": "5B3hGqhNaNkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using tfidf\n",
        "#using tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "t_vectorizer = TfidfVectorizer(max_df = 0.9,min_df = 0.01, max_features=5000)\n",
        "X= t_vectorizer.fit_transform(words)"
      ],
      "metadata": {
        "id": "rs1JM2zUaaoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "cW3uIv5Ram7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying PCA-Principal Component Analysis to reduce dimensions."
      ],
      "metadata": {
        "id": "mLPsGWZSbEee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA Code\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "transformer = PCA()\n",
        "transformer.fit(X.toarray())"
      ],
      "metadata": {
        "id": "K5PuIH8Q18mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #explained var v/s comp\n",
        "plt.plot(np.cumsum(transformer.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')"
      ],
      "metadata": {
        "id": "W9fcega82ik4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Adjust the parameters of TfidfVectorizer\n",
        "# Try reducing min_df or increasing max_features\n",
        "t_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.001, max_features=5000)\n",
        "X = t_vectorizer.fit_transform(words)\n",
        "\n",
        "# Check the shape of X after vectorization\n",
        "print(\"Shape of X after TF-IDF:\", X.shape)\n",
        "\n",
        "# If X.shape[1] is still 0, it means no features were extracted\n",
        "if X.shape[1] == 0:\n",
        "    print(\"No features were extracted by TF-IDF. Please adjust the parameters.\")\n",
        "else:\n",
        "    # Proceed with PCA\n",
        "    from sklearn.decomposition import PCA\n",
        "    transformer = PCA(n_components=0.95)\n",
        "    transformer.fit(X.toarray())\n",
        "    X_transformed = transformer.transform(X.toarray())\n",
        "    print(\"Shape of X_transformed:\", X_transformed.shape)"
      ],
      "metadata": {
        "id": "sm8qnL9g2yji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizing the test and train\n",
        "X_vectorized = t_vectorizer.transform(words)"
      ],
      "metadata": {
        "id": "pZBnYW-yjXdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying pca\n",
        "X= transformer.transform(X_vectorized.toarray())"
      ],
      "metadata": {
        "id": "XNtaEE5P3VCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X\n"
      ],
      "metadata": {
        "id": "oc-kTbwmjgD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Model Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "VzQehU1MjqiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will plot the graph to get the no. of clusters\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "wcss = []\n",
        "for i in range(1, 60):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=200, n_init=10, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(range(1, 60), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jfZytIyXFNZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples"
      ],
      "metadata": {
        "id": "84bdqvpf50Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,12))\n",
        "plt.title('dendrogram')\n",
        "dend = dendrogram(linkage(X, method='ward'))"
      ],
      "metadata": {
        "id": "o2wCzmWU1G6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AgglomerativeClustering**"
      ],
      "metadata": {
        "id": "BK8wx1Wq4j16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster = AgglomerativeClustering(n_clusters=355)\n",
        "labels_ = cluster.fit_predict(X)\n",
        "labels_"
      ],
      "metadata": {
        "id": "UvQ2mM9w4iBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score(X, labels_)"
      ],
      "metadata": {
        "id": "0ObruwBp4xKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Silhouette Score for each cluster\n",
        "silhouette_score_ = [  ]\n",
        "range_n_clusters = [i for i in range(2,55)]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    #centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    silhouette_score_.append([int(n_clusters) , round(score , 3)])\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "7IBCd6dU4zMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,8))\n",
        "plt.scatter(X[:,0],X[:,1],c=cluster.labels_, cmap='rainbow')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uydy08c6450D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KMeans Clusters**"
      ],
      "metadata": {
        "id": "Lktc9Y0v5E4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster = KMeans(n_clusters=395, random_state=0)\n",
        "y_pred = cluster.fit_predict(X)\n",
        "\n",
        "silhouette_score(X, y_pred)"
      ],
      "metadata": {
        "id": "iL5I_HIq5Dv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_samples(X, y_pred)"
      ],
      "metadata": {
        "id": "wCysLbAX5KmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(X)\n",
        "\n",
        "#Getting unique labels\n",
        "u_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in u_labels:\n",
        "    plt.scatter(X[label == i , 0] , X[label == i , 1] , label = i)\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hmy2zSdu5PV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- Data Overview\n",
        "1- Data Overview\n",
        "\n",
        "We have 533645 rows and 8 columns provided in the data.\n",
        "\n",
        "In the dataset we have 2 object columns and 6 integer column as year.\n",
        "\n",
        "2- Checking the null values\n",
        "\n",
        "InvoiceNo : Count : 0 : Percentage : 0.0\n",
        "\n",
        "StockCode : Count : 0 : Percentage : 0.0\n",
        "\n",
        "Description : Count : 0 : Percentage : 0.0\n",
        "\n",
        "Quantity : Count : 0 : Percentage : 0.0\n",
        "\n",
        "InvoiceDate : Count : 0 : Percentage : 0.0\n",
        "\n",
        "UnitPrice : Count : 0 : Percentage : 0.0\n",
        "\n",
        "CustomerID : Count : 135037 : Percentage : 25.16\n",
        "\n",
        "Country : Count : 0 : Percentage : 0.0\n",
        "\n",
        "Fist we have 135037 null values in customerid column.We have almost 25% null values in this column so we can not use this column in model training but we can use it in EDA.\n",
        "3- Check Duplicate values in the dataset\n",
        "\n",
        "we do not have any Duplicate values in the dataset.\n",
        "Number of Unique : Country : 38\n",
        "\n",
        "Number of Unique : Description : 4224\n",
        "\n",
        "Number of Unique : Quantity : 722\n",
        "\n",
        "Number of Unique : UnitPrice : 1630\n",
        "\n",
        "Number of Unique : year : 2\n",
        "\n",
        "Number of Unique : month : 12\n",
        "\n",
        "Number of Unique : month_name : 12\n",
        "\n",
        "Number of Unique : week_name : 6\n",
        "\n",
        "Number of Unique : quarter : 4\n",
        "\n",
        "Number of Unique : days : 31\n",
        "\n",
        "Number of Unique : week : 6\n",
        "\n",
        "Number of Unique : hour : 15\n",
        "\n",
        "Number of Unique : minute : 60\n",
        "\n",
        "2- Data pre-processing\n",
        "1- Feature Engineering\n",
        "\n",
        "For train the model we use description column and Country column.\n",
        "2- We performe Text cleaning as our next step\n",
        "\n",
        "convert all words in lowercase.\n",
        "3- We performe Stemming as our next step\n",
        "\n",
        "We remove all stopwords.\n",
        "Also use stemming function.\n",
        "4- We performe TF-IDF vectorizer\n",
        "\n",
        "Term frequency-inverse document frequency is a text vectorizer that transforms the text into a usable vector.\n",
        "5- Applying PCA-Principal Component Analysis to reduce dimensions.\n",
        "\n",
        "We will use 2000 components\n",
        "3- Applying models\n",
        "1- Find the value of clusters\n",
        "\n",
        "WE use Elbow method for finding k values.\n",
        "Also use Silhouette Score for best score.\n",
        "Also use Dendogram for finding the value of clusters.\n",
        "2- Use Agglomerative Clustering\n",
        "\n",
        "3- Use KMeans Clustering\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}